# NDAIVI Windsurf Rules

## Project Structure
- Keep all Python code in appropriate modules (scraper/, database/)
- Maintain clear separation of concerns between components
- Use relative imports within the project
- Refer to readme.md for context on the project structure and files.

## Coding Standards
- Follow PEP 8 for Python style (4-space indents, snake_case for variables)
- Use type hints where possible (e.g., def scrape(url: str) -> list)
- Add docstrings to all functions and classes
- Use logging module instead of print statements

## Database Guidelines
- Always use the db_manager singleton for database access
- Use context managers for database sessions
- Handle database exceptions appropriately
- Validate data before inserting into the database

## AI Integration
- Maintain strict delimited format for Claude API responses
- Keep prompt templates in the prompt_templates.py file
- Handle API rate limits and errors gracefully

## Error Handling
- Log all errors with appropriate context
- Use try/except blocks for external API calls
- Implement retries for transient errors

## Configuration
- Store sensitive information in .env file (not in config.yaml)
- Document all configuration parameters
- Validate configuration before use

## Testing
- Write unit tests for critical components
- Test error handling paths
- Mock external API calls in tests

## File Descriptions

### Root Directory
- `main.py`: Entry point for the scraper application. Handles command-line arguments, configuration validation, and initializes the scraping process.
- `run_claude_scraper.py`: Specialized script for running Claude-based scraping operations independently.
- `cli.py`: Command-line interface providing various commands to interact with the scraper and database.
- `utils.py`: Utility functions used across the application for common tasks.
- `validate_data.py`: Tools for validating and correcting data in the database.
- `config.yaml`: Configuration file containing settings for the scraper, database, AI APIs, and crawling parameters.
- `README.md`: Documentation of the project, its structure, and usage instructions.

### Database Module
- `database/__init__.py`: Package initialization for database module.
- `database/db_manager.py`: Singleton database manager that handles connections and prevents race conditions.
- `database/schema.py`: Defines the SQLAlchemy ORM models and database schema, including tables for manufacturers, categories, products, and logging.

### Scraper Module
- `scraper/__init__.py`: Package initialization for scraper module.
- `scraper/competitor_scraper.py`: Main implementation of the competitor website scraper. Handles crawling, URL management, and data extraction.
- `scraper/claude_analyzer.py`: Integration with Claude AI for analyzing web content, detecting manufacturer pages, and extracting structured data.
- `scraper/prompt_templates.py`: Contains prompt templates for Claude AI to ensure consistent and reliable data extraction.

### Logs Directory
- `logs/ndaivi.log`: Main application log file.
- `logs/competitor_scraper.log`: Detailed logs specific to the competitor scraping process.

# Tasks for Today's Programming Session March 22 2025 (Sprint 2):
-The scraper should be fleshed out completely, that means:
Efficient logic, concise prompts, and cost-effective operations.
Low / inexistent false positives
Able to start-stop-resume with no issue
Able to run in the background with no issue
Functioning translation system
Functioning statistics system.

-We should be able to use the website finder without errors, with the same requirements as the scraper, using an improved logic.
-We should be able to run both sequentially using the "run-all" command, run in the background, and consult progress.

Only when this is finished, we should start working on the manuals crawler.

NEW LOGIC SYSTEM:
For manufacturer detection:

Follow a 4-step analysis:
First analyse title, headers and metadata with keywords (No claude), to DISCARD useless pages. The system should be comprehensive enough to not discard any brand page and quickly discard useless pages.
If it passes, gets sent to claude, analysing title and metadata
Then it analyses href links, nav, breadcrumbs, etc. 
If that does not work, sends a truncated page to claude, as we are currently doing now.

THe question that will be answered by Claude has three possible answers: Is this a brand page, a brand category page, or other?

Brand page: It uses the full analysis we have already coded (Taking all titles, headers, metadata, keywords, etc) to extract all categories.
Brand category page: It analyses headers only (), and adds the category if it is not already present.
Other page: Skipped completely. 
Inconclusive: It moves to the next step.


CURRENT ISSUES:
Stats manager is logging phantom sessions, when trying to use it, creating "sessions" with no data when just trying to see statistics.
Translation system not being called after finding categories
We need a better logic, that was previously prposed. 
We should adjust prompts to avoid false positives, avoid unnessary comments and conserve tokens, meaning a lower operating cost.
Remove "Category validation" with Claude, it consumes unnecessary resources. It can be handled by better prompting.

This should greatly increase the speed of the system, while the website finder is a much simpler implementation, that will only google (or asks the ai) for the official website where manuals can be located in each target language. 

With that approach, we will have a solid enough base to build the manuals finding engine, which is the core of this project.