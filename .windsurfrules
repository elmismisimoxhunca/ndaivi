# NDAIVI Windsurf Rules

## Project Structure
- Keep all Python code in appropriate modules (scraper/, database/)
- Maintain clear separation of concerns between components
- Use relative imports within the project
- Refer to readme.md for context on the project structure and files.

## Coding Standards
- Follow PEP 8 for Python style (4-space indents, snake_case for variables)
- Use type hints where possible (e.g., def scrape(url: str) -> list)
- Add docstrings to all functions and classes
- Use logging module instead of print statements

## Database Guidelines
- Always use the db_manager singleton for database access
- Use context managers for database sessions
- Handle database exceptions appropriately
- Validate data before inserting into the database

## AI Integration
- Maintain strict delimited format for Claude API responses
- Keep prompt templates in the prompt_templates.py file
- Handle API rate limits and errors gracefully

## Error Handling
- Log all errors with appropriate context
- Use try/except blocks for external API calls
- Implement retries for transient errors

## Configuration
- Store sensitive information in .env file (not in config.yaml)
- Document all configuration parameters
- Validate configuration before use

## Testing
- Write unit tests for critical components
- Test error handling paths
- Mock external API calls in tests

## File Descriptions

### Root Directory
- `main.py`: Entry point for the scraper application. Handles command-line arguments, configuration validation, and initializes the scraping process.
- `run_claude_scraper.py`: Specialized script for running Claude-based scraping operations independently.
- `cli.py`: Command-line interface providing various commands to interact with the scraper and database.
- `utils.py`: Utility functions used across the application for common tasks.
- `validate_data.py`: Tools for validating and correcting data in the database.
- `config.yaml`: Configuration file containing settings for the scraper, database, AI APIs, and crawling parameters.
- `README.md`: Documentation of the project, its structure, and usage instructions.

### Database Module
- `database/__init__.py`: Package initialization for database module.
- `database/db_manager.py`: Singleton database manager that handles connections and prevents race conditions.
- `database/schema.py`: Defines the SQLAlchemy ORM models and database schema, including tables for manufacturers, categories, products, and logging.

### Scraper Module
- `scraper/__init__.py`: Package initialization for scraper module.
- `scraper/competitor_scraper.py`: Main implementation of the competitor website scraper. Handles crawling, URL management, and data extraction.
- `scraper/claude_analyzer.py`: Integration with Claude AI for analyzing web content, detecting manufacturer pages, and extracting structured data.
- `scraper/prompt_templates.py`: Contains prompt templates for Claude AI to ensure consistent and reliable data extraction.

### Logs Directory
- `logs/ndaivi.log`: Main application log file.
- `logs/competitor_scraper.log`: Detailed logs specific to the competitor scraping process.

# Tasks for Today's Programming Session March 21 2025 (Sprint 2):
-Fully flesh out the STATS MANAGER so it properly displays session statistics, for scraper, finder, and combined.
-We should be able to run the scraper without errors , even on background operation, and see the past sessions as well as global stats.
-We should be able to use the website finder without errors.
-We should be able to run both sequentially using the "run-all" command, run in the background, and consult progress.

CURRENT ISSUES:

ndaivi> scrape --max-pages 4
Starting competitor scraper...
2025-03-21 21:06:30,102 - competitor_scraper - INFO - Configuration loaded from config.yaml
2025-03-21 21:06:30,102 - db_manager - INFO - Initializing database connection to database/manuals.db
2025-03-21 21:06:30,108 - db_manager - INFO - SQLite pragmas set successfully
2025-03-21 21:06:30,108 - db_manager - INFO - Database connection initialized successfully
2025-03-21 21:06:30,108 - competitor_scraper - INFO - Database manager initialized for database/manuals.db
2025-03-21 21:06:30,108 - cli - ERROR - Error in interactive mode: Client.__init__() got an unexpected keyword argument 'proxies'
Error: Client.__init__() got an unexpected keyword argument 'proxies'

This is an error we have encountered before, and its caused by CLAUDE unknowingly coding with an old version of Anthropic SDK, the "proxies" field is deprecated, cause we need the functionaltiy, we need to bypass the SDK and call the API Directly.